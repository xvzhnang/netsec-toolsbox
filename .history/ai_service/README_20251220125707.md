# AI Gateway Service

基于 One API 架构思想的轻量级 AI Gateway 实现。

## 架构设计

```
models.json (配置)
    ↓
Registry (加载配置，创建 Adapter 实例)
    ↓
Router (根据 model_id 查找 Adapter)
    ↓
Adapter (发送请求，转换响应)
    ↓
OpenAI-compatible Response
```

## 目录结构

```text
ai_service/
├── main_gateway.py          # 主服务入口
├── requirements.txt          # Python 依赖
├── test_gateway.py          # 测试脚本
├── README.md                # 本文档
├── .gitignore               # Git 忽略文件
├── api/                     # API 层
│   └── openai_handler.py    # OpenAI-compatible API Handler
├── core/                    # 核心模块
│   ├── adapter/             # 适配器模块
│   │   ├── base_adapter.py              # 适配器基类
│   │   ├── openai_compat_adapter.py     # OpenAI 兼容适配器
│   │   ├── custom_http_adapter.py       # Custom HTTP 适配器
│   │   ├── process_adapter.py           # Process 适配器
│   │   ├── websocket_adapter.py         # WebSocket 适配器基类
│   │   ├── xunfei_adapter.py            # 讯飞星火适配器
│   │   └── converters/                  # 协议转换器
│   │       ├── base_converter.py        # 转换器基类
│   │       ├── registry.py              # 转换器注册表
│   │       └── *_converter.py           # 各种协议转换器
│   ├── registry.py          # 模型注册表
│   ├── router.py            # 请求路由器
│   └── retry.py             # 错误重试机制
├── config/                  # 配置文件
│   ├── models.json          # 模型配置
│   └── models.json.example  # 配置示例
└── docs/                    # 文档目录
    ├── README.md            # 文档索引
    └── *.md                 # 详细文档
```

## 快速开始

### 1. 安装依赖

```bash
cd ai_service
python -m pip install -r requirements.txt
```

### 2. 配置模型

编辑 `config/models.json`，添加你的 API Key：

```json
{
  "models": [
    {
      "id": "gpt-3.5-turbo",
      "adapter": "openai_compat",
      "base_url": "https://api.openai.com/v1",
      "api_key": "ENV:OPENAI_API_KEY",
      "enabled": true,
      "model": "gpt-3.5-turbo"
    }
  ]
}
```

### 3. 启动服务

```bash
python main_gateway.py --port 8765
```

### 4. 测试

```bash
# 获取模型列表
curl http://127.0.0.1:8765/v1/models

# 发送聊天请求
curl http://127.0.0.1:8765/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

## 配置说明

### 完整模型列表

AI Gateway 支持 One API 中所有已实现的 AI 模型，包括：

**OpenAI 兼容模型**（使用 `openai_compat` 适配器）：

- OpenAI (GPT-3.5, GPT-4, GPT-4 Turbo)
- DeepSeek (Chat, Coder)
- Groq
- Together AI
- Mistral AI
- Ollama (本地)
- LM Studio (本地)
- vLLM (本地)
- LocalAI (本地)
- Cloudflare Workers AI
- 360 智脑（AI360）
- Moonshot (Kimi)
- Baichuan (百川智能)
- 零一万物（LingYiWanWu）
- 阶跃星辰（StepFun）
- SiliconFlow
- xAI (Grok)
- Novita
- OpenRouter
- 百度文心一言 V2
- Google Gemini V2
- 讯飞星火 V2

**需要协议转换的模型**（使用 `custom_http` 适配器）：

- Anthropic Claude
- Google Gemini
- 智谱 AI (Zhipu)
- 百度文心一言
- 阿里通义千问
- 讯飞星火
- 腾讯混元
- Moonshot (Kimi)
- 百川智能
- MINIMAX
- 字节跳动豆包
- 零一万物
- 阶跃星辰
- Cohere
- Coze
- SiliconFlow
- xAI (Grok)
- Replicate
- DeepL
- Novita
- 360 智脑
- Google Vertex AI
- AWS Bedrock
- 阿里百炼

详细的模型配置和使用方法请参考 [MODELS_CONFIG_GUIDE.md](./MODELS_CONFIG_GUIDE.md)。

### models.json 格式

```json
{
  "models": [
    {
      "id": "模型ID（用于路由）",
      "adapter": "适配器类型（openai_compat / custom_http / process）",
      "base_url": "API 基础 URL",
      "api_key": "API Key（支持 ENV:VAR_NAME 环境变量）",
      "enabled": true,
      "model": "实际使用的模型名称",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    }
  ]
}
```

### 环境变量支持

API Key 支持环境变量引用：
- `"api_key": "ENV:OPENAI_API_KEY"` - 从环境变量 `OPENAI_API_KEY` 读取

## API 接口

### POST /v1/chat/completions

OpenAI-compatible 聊天接口。

**请求示例**：
```json
{
  "model": "gpt-3.5-turbo",
  "messages": [
    {"role": "user", "content": "Hello!"}
  ],
  "temperature": 0.7,
  "max_tokens": 2000
}
```

**响应格式**：标准 OpenAI Chat Completions 响应

### GET /v1/models

获取可用模型列表。

**响应示例**：
```json
{
  "object": "list",
  "data": [
    {
      "id": "gpt-3.5-turbo",
      "object": "model",
      "created": 0,
      "owned_by": "openai_compat"
    }
  ]
}
```

### GET /health

健康检查接口。

## 适配器类型

### openai_compat

适用于所有 OpenAI-compatible API：

- OpenAI、Azure OpenAI
- DeepSeek
- Groq、Together AI、Mistral AI
- Ollama、LM Studio、vLLM、LocalAI
- Cloudflare Workers AI
- 360 智脑、Moonshot、Baichuan 等

### custom_http

适用于需要协议转换的 API：

- Anthropic Claude
- Google Gemini
- 智谱 AI、百度文心一言、阿里通义千问
- 腾讯混元、Moonshot、MINIMAX
- 字节跳动豆包、Cohere、Coze
- DeepL 等

### websocket

适用于 WebSocket 协议的模型：

- 讯飞星火

### process

适用于本地 CLI 工具：

- llama.cpp
- Ollama CLI
- 自定义脚本

## 与 One API 的对比

| 特性 | One API | AI Gateway |
|------|---------|------------|
| 接口 | OpenAI-compatible | ✅ 相同 |
| 配置存储 | 数据库 | ✅ JSON 文件 |
| 用户系统 | ✅ 完整 | ❌ 无 |
| 配额管理 | ✅ 完整 | ❌ 无 |
| Web 管理后台 | ✅ 完整 | ❌ 无 |
| 适配器抽象 | ✅ | ✅ 相同 |
| 复杂度 | 高 | 低 |

## 开发计划

- [x] OpenAI Compat 适配器
- [x] Registry 和 Router
- [x] API Handler
- [x] 完整的模型配置（基于 One API）
- [x] Custom HTTP 适配器（支持 Anthropic, Gemini, Zhipu, Baidu, Ali）
- [x] 更多转换器（腾讯、Moonshot、Minimax、Doubao）
- [x] Process 适配器（本地 CLI 工具）
- [x] 流式响应支持（SSE）
- [x] 错误重试机制
- [x] WebSocket 支持（讯飞星火）

## 相关文档

### 配置与使用
- [模型配置指南](./docs/MODELS_CONFIG_GUIDE.md) - 所有支持的模型及其配置方法
- [配置迁移指南](./docs/CONFIG_MIGRATION.md) - 从 One API 迁移配置

### 实现文档
- [Custom HTTP 适配器实现](./docs/CUSTOM_HTTP_ADAPTER_IMPLEMENTATION.md) - Custom HTTP 适配器详细说明
- [Process 适配器指南](./docs/PROCESS_ADAPTER_GUIDE.md) - 本地 CLI 工具支持
- [流式响应指南](./docs/STREAMING_GUIDE.md) - SSE 流式响应实现
- [WebSocket 支持指南](./docs/WEBSOCKET_GUIDE.md) - WebSocket 协议支持
- [错误重试机制](./docs/RETRY_GUIDE.md) - 重试策略与配置

### 状态与架构
- [实现状态对比](./docs/IMPLEMENTATION_STATUS.md) - 与 One API 的对比
- [代码结构说明](./docs/CODE_STRUCTURE.md) - 代码组织架构
- [代码审计报告](./docs/CODE_AUDIT_REPORT.md) - 代码质量审计

### 外部参考
- [架构分析文档](../ONE_API_ARCHITECTURE_ANALYSIS.md) - One API 架构分析
- [实现指南](../AI_GATEWAY_IMPLEMENTATION_GUIDE.md) - 实现细节和代码示例
