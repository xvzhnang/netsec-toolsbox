{
  "_comment": "AI Gateway 模型配置文件",
  "_note": "基于 One API 的实现，包含所有支持的 AI 模型配置",
  "_usage": "1. 复制 models.json.example 中的配置到此处 2. 设置环境变量 3. 将需要使用的模型的 enabled 设置为 true",
  "models": [
    {
      "_comment": "=== OpenAI 兼容模型（使用 openai_compat 适配器）===",
      "_note": "这些模型使用 OpenAI-compatible API，可以直接使用"
    },
    {
      "id": "gpt-3.5-turbo",
      "adapter": "openai_compat",
      "base_url": "https://api.openai.com/v1",
      "api_key": "ENV:OPENAI_API_KEY",
      "enabled": false,
      "model": "gpt-3.5-turbo",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "gpt-4",
      "adapter": "openai_compat",
      "base_url": "https://api.openai.com/v1",
      "api_key": "ENV:OPENAI_API_KEY",
      "enabled": false,
      "model": "gpt-4",
      "temperature": 0.7,
      "max_tokens": 4000,
      "timeout": 120
    },
    {
      "id": "deepseek-chat",
      "adapter": "openai_compat",
      "base_url": "https://api.deepseek.com/v1",
      "api_key": "sk-8219e24a004c445985e4c291a7a15698",
      "enabled": true,
      "model": "deepseek-chat",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "deepseek-coder",
      "adapter": "openai_compat",
      "base_url": "https://api.deepseek.com/v1",
      "api_key": "ENV:DEEPSEEK_API_KEY",
      "enabled": false,
      "model": "deepseek-coder",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "groq-llama3",
      "adapter": "openai_compat",
      "base_url": "https://api.groq.com/openai/v1",
      "api_key": "ENV:GROQ_API_KEY",
      "enabled": false,
      "model": "llama-3.1-70b-versatile",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "together-llama",
      "adapter": "openai_compat",
      "base_url": "https://api.together.xyz/v1",
      "api_key": "ENV:TOGETHER_API_KEY",
      "enabled": false,
      "model": "meta-llama/Llama-3-70b-chat-hf",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "mistral-large",
      "adapter": "openai_compat",
      "base_url": "https://api.mistral.ai/v1",
      "api_key": "ENV:MISTRAL_API_KEY",
      "enabled": false,
      "model": "mistral-large-latest",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "ollama-llama3",
      "adapter": "openai_compat",
      "base_url": "http://localhost:11434/v1",
      "api_key": "not-needed",
      "enabled": false,
      "model": "llama3",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 120
    },
    {
      "id": "lmstudio-local",
      "adapter": "openai_compat",
      "base_url": "http://localhost:1234/v1",
      "api_key": "not-needed",
      "enabled": false,
      "model": "local-model",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 120
    },
    {
      "id": "vllm-local",
      "adapter": "openai_compat",
      "base_url": "http://localhost:8000/v1",
      "api_key": "not-needed",
      "enabled": false,
      "model": "local-model",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 120
    },
    {
      "id": "localai",
      "adapter": "openai_compat",
      "base_url": "http://localhost:8080/v1",
      "api_key": "not-needed",
      "enabled": false,
      "model": "local-model",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 120
    },
    {
      "_comment": "=== 需要协议转换的模型（使用 custom_http 适配器，待实现）===",
      "_note": "这些模型的配置已准备好，等待 custom_http 适配器实现",
      "_warning": "custom_http 适配器尚未实现，这些模型暂时无法使用"
    },
    {
      "id": "claude-3-5-sonnet",
      "adapter": "custom_http",
      "base_url": "https://api.anthropic.com",
      "api_key": "ENV:ANTHROPIC_API_KEY",
      "enabled": false,
      "model": "claude-3-5-sonnet-20241022",
      "endpoint": "/v1/messages",
      "request_format": "anthropic",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "gemini-pro",
      "adapter": "custom_http",
      "base_url": "https://generativelanguage.googleapis.com",
      "api_key": "ENV:GEMINI_API_KEY",
      "enabled": false,
      "model": "gemini-pro",
      "endpoint": "/v1beta/models/{model}:generateContent",
      "request_format": "gemini",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "zhipu-glm-4",
      "adapter": "custom_http",
      "base_url": "https://open.bigmodel.cn/api/paas/v4",
      "api_key": "ENV:ZHIPU_API_KEY",
      "enabled": false,
      "model": "glm-4",
      "endpoint": "/chat/completions",
      "request_format": "zhipu",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "baidu-ernie-4.0",
      "adapter": "custom_http",
      "base_url": "https://aip.baidubce.com",
      "api_key": "ENV:BAIDU_API_KEY",
      "enabled": false,
      "model": "ERNIE-4.0-8K",
      "endpoint": "/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/completions",
      "request_format": "baidu",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60,
      "config": {
        "access_token": "ENV:BAIDU_ACCESS_TOKEN"
      }
    },
    {
      "id": "ali-qwen-max",
      "adapter": "custom_http",
      "base_url": "https://dashscope.aliyuncs.com",
      "api_key": "ENV:ALI_API_KEY",
      "enabled": false,
      "model": "qwen-max",
      "endpoint": "/api/v1/services/aigc/text-generation/generation",
      "request_format": "ali",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "_comment": "=== WebSocket 模型（使用 websocket 适配器）===",
      "_note": "这些模型使用 WebSocket 协议进行通信"
    },
    {
      "id": "xunfei-spark-max",
      "adapter": "websocket",
      "base_url": "wss://spark-api.xf-yun.com",
      "api_key": "ENV:XUNFEI_API_KEY",
      "enabled": false,
      "model": "Spark-Max",
      "endpoint": "/v3.5/chat",
      "request_format": "xunfei",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60,
      "config": {
        "app_id": "ENV:XUNFEI_APP_ID",
        "api_secret": "ENV:XUNFEI_API_SECRET",
        "api_version": "v3.5",
        "domain": "generalv3.5"
      }
    },
    {
      "id": "tencent-hunyuan-pro",
      "adapter": "custom_http",
      "base_url": "https://hunyuan.tencentcloudapi.com",
      "api_key": "ENV:TENCENT_SECRET_ID",
      "enabled": false,
      "model": "hunyuan-pro",
      "endpoint": "/",
      "request_format": "tencent",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60,
      "config": {
        "secret_key": "ENV:TENCENT_SECRET_KEY",
        "region": "ap-beijing"
      }
    },
    {
      "id": "moonshot-kimi",
      "adapter": "custom_http",
      "base_url": "https://api.moonshot.cn/v1",
      "api_key": "ENV:MOONSHOT_API_KEY",
      "enabled": false,
      "model": "moonshot-v1-8k",
      "endpoint": "/chat/completions",
      "request_format": "moonshot",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "baichuan-turbo",
      "adapter": "custom_http",
      "base_url": "https://api.baichuan-ai.com/v1",
      "api_key": "ENV:BAICHUAN_API_KEY",
      "enabled": false,
      "model": "Baichuan2-Turbo",
      "endpoint": "/chat/completions",
      "request_format": "baichuan",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "minimax-abab5",
      "adapter": "custom_http",
      "base_url": "https://api.minimax.chat/v1",
      "api_key": "ENV:MINIMAX_API_KEY",
      "enabled": false,
      "model": "abab5.5-chat",
      "endpoint": "/text/chatcompletion_v2",
      "request_format": "minimax",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60,
      "config": {
        "group_id": "ENV:MINIMAX_GROUP_ID"
      }
    },
    {
      "id": "doubao-pro",
      "adapter": "custom_http",
      "base_url": "https://ark.cn-beijing.volces.com/api/v3",
      "api_key": "ENV:DOUBAO_API_KEY",
      "enabled": false,
      "model": "doubao-pro-32k",
      "endpoint": "/chat/completions",
      "request_format": "doubao",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "lingyiwanwu-yi-34b",
      "adapter": "openai_compat",
      "base_url": "https://api.lingyiwanwu.com/v1",
      "api_key": "ENV:LINGYIWANWU_API_KEY",
      "enabled": false,
      "model": "yi-34b-chat-0205",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "stepfun-step-1",
      "adapter": "openai_compat",
      "base_url": "https://api.stepfun.com/v1",
      "api_key": "ENV:STEPFUN_API_KEY",
      "enabled": false,
      "model": "step-1-32k",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "cohere-command",
      "adapter": "custom_http",
      "base_url": "https://api.cohere.ai/v1",
      "api_key": "ENV:COHERE_API_KEY",
      "enabled": false,
      "model": "command",
      "endpoint": "/chat",
      "request_format": "cohere",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "coze-chat",
      "adapter": "custom_http",
      "base_url": "https://api.coze.cn/open_api/v2",
      "api_key": "ENV:COZE_API_KEY",
      "enabled": false,
      "model": "coze-chat",
      "endpoint": "/chat",
      "request_format": "coze",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "siliconflow-llama",
      "adapter": "custom_http",
      "base_url": "https://api.siliconflow.cn/v1",
      "api_key": "ENV:SILICONFLOW_API_KEY",
      "enabled": false,
      "model": "meta-llama/Llama-3-70B-Instruct",
      "endpoint": "/chat/completions",
      "request_format": "siliconflow",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "xai-grok",
      "adapter": "custom_http",
      "base_url": "https://api.x.ai/v1",
      "api_key": "ENV:XAI_API_KEY",
      "enabled": false,
      "model": "grok-beta",
      "endpoint": "/chat/completions",
      "request_format": "xai",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "replicate-llama",
      "adapter": "custom_http",
      "base_url": "https://api.replicate.com/v1",
      "api_key": "ENV:REPLICATE_API_KEY",
      "enabled": false,
      "model": "meta/llama-2-70b-chat",
      "endpoint": "/predictions",
      "request_format": "replicate",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 120
    },
    {
      "id": "deepl-translate",
      "adapter": "custom_http",
      "base_url": "https://api-free.deepl.com",
      "api_key": "ENV:DEEPL_API_KEY",
      "enabled": false,
      "model": "deepl-translate",
      "endpoint": "/v2/translate",
      "request_format": "deepl",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "novita-image",
      "adapter": "custom_http",
      "base_url": "https://api.novita.ai/v3",
      "api_key": "ENV:NOVITA_API_KEY",
      "enabled": false,
      "model": "novita-image",
      "endpoint": "/image/generation",
      "request_format": "novita",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "ai360-360gpt",
      "adapter": "custom_http",
      "base_url": "https://api.360.cn/v1",
      "api_key": "ENV:AI360_API_KEY",
      "enabled": false,
      "model": "360gpt-pro",
      "endpoint": "/chat/completions",
      "request_format": "ai360",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "id": "vertexai-gemini",
      "adapter": "custom_http",
      "base_url": "https://{REGION}-aiplatform.googleapis.com",
      "api_key": "ENV:VERTEXAI_CREDENTIALS",
      "enabled": false,
      "model": "gemini-pro",
      "endpoint": "/v1/projects/{PROJECT_ID}/locations/{REGION}/publishers/google/models/{model}:predict",
      "request_format": "vertexai",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60,
      "config": {
        "project_id": "ENV:VERTEXAI_PROJECT_ID",
        "region": "us-central1"
      }
    },
    {
      "id": "aws-claude",
      "adapter": "custom_http",
      "base_url": "https://bedrock-runtime.{REGION}.amazonaws.com",
      "api_key": "ENV:AWS_ACCESS_KEY_ID",
      "enabled": false,
      "model": "anthropic.claude-3-5-sonnet-20241022-v2:0",
      "endpoint": "/model/{model}/invoke",
      "request_format": "aws",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60,
      "config": {
        "secret_access_key": "ENV:AWS_SECRET_ACCESS_KEY",
        "region": "us-east-1"
      }
    },
    {
      "id": "alibailian-qwen",
      "adapter": "custom_http",
      "base_url": "https://dashscope.aliyuncs.com",
      "api_key": "ENV:ALI_API_KEY",
      "enabled": false,
      "model": "qwen-max",
      "endpoint": "/api/v1/services/aigc/text-generation/generation",
      "request_format": "alibailian",
      "response_format": "openai",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout": 60
    },
    {
      "_comment": "=== 本地 CLI 工具（使用 process 适配器）===",
      "_note": "这些模型通过本地命令行工具运行"
    },
    {
      "id": "local-llama-cpp",
      "adapter": "process",
      "command": "llama-cli",
      "args": [
        "-m",
        "ENV:LLAMA_MODEL_PATH",
        "--ctx-size",
        "4096",
        "--temp",
        "0.7"
      ],
      "enabled": false,
      "input_format": "prompt",
      "output_format": "text",
      "timeout": 120,
      "working_dir": null,
      "env": null
    },
    {
      "id": "local-ollama-cli",
      "adapter": "process",
      "command": "ollama",
      "args": [
        "run",
        "llama3"
      ],
      "enabled": false,
      "input_format": "prompt",
      "output_format": "text",
      "timeout": 120,
      "working_dir": null,
      "env": null
    },
    {
      "id": "local-custom-json",
      "adapter": "process",
      "command": "python",
      "args": [
        "-c",
        "import sys, json; data=json.load(sys.stdin); print(json.dumps({'content': f'Response to: {data.get(\"messages\", [{}])[-1].get(\"content\", \"\")}'}))"
      ],
      "enabled": false,
      "input_format": "json",
      "output_format": "json",
      "timeout": 60,
      "working_dir": null,
      "env": null
    }
  ]
}