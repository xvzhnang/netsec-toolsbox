# -*- coding: utf-8 -*-
"""
æ¨¡å‹æ³¨å†Œè¡¨
å¯¹åº” One API çš„ model/channel.go + relay/adaptor.go
è´Ÿè´£ä»é…ç½®æ–‡ä»¶åŠ è½½æ¨¡å‹ï¼Œç®¡ç† Adapter å®ä¾‹
"""
import json
import os
import sys
import io
import traceback
from typing import Dict, Optional, List, Any
from pathlib import Path

# Windows ç¼–ç ä¿®å¤
# æ³¨æ„ï¼šä¸è¦åœ¨è¿™é‡Œé‡æ–°åŒ…è£… stdout/stderrï¼Œå› ä¸ºå¯èƒ½ä¼šä¸ç®¡é“é‡å®šå‘å†²çª
# ç¼–ç é—®é¢˜ç”±è°ƒç”¨æ–¹ï¼ˆmain_gateway.pyï¼‰å¤„ç†

# æ·»åŠ  ai_service ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.adapter.base_adapter import ChatAdapter
from core.adapter.openai_compat_adapter import OpenAICompatAdapter
from core.adapter.custom_http_adapter import CustomHTTPAdapter
from core.adapter.process_adapter import ProcessAdapter
from core.adapter.xunfei_adapter import XunfeiAdapter


def safe_print(*args, **kwargs):
    """å®‰å…¨æ‰“å°å‡½æ•°ï¼Œåœ¨ stdout/stderr ä¸å¯ç”¨æ—¶è·³è¿‡"""
    try:
        # å¦‚æœæŒ‡å®šäº† file å‚æ•°ï¼Œä½¿ç”¨æŒ‡å®šçš„æµï¼Œå¦åˆ™ä½¿ç”¨ stdout
        file = kwargs.pop('file', sys.stdout)
        if file and hasattr(file, 'closed') and not file.closed:
            print(*args, file=file, flush=True, **kwargs)
        elif file and not hasattr(file, 'closed'):
            # æŸäº›æµå¯èƒ½æ²¡æœ‰ closed å±æ€§
            print(*args, file=file, flush=True, **kwargs)
    except (ValueError, OSError, AttributeError):
        # æµå·²å…³é—­æˆ–ä¸å¯ç”¨ï¼Œè·³è¿‡è¾“å‡º
        pass


class ModelRegistry:
    """æ¨¡å‹æ³¨å†Œè¡¨"""
    
    def __init__(self, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–æ¨¡å‹æ³¨å†Œè¡¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨æŸ¥æ‰¾
        """
        self.config_path = config_path or self._find_config_path()
        self.adapters: Dict[str, ChatAdapter] = {}
        self._load_models()
    
    def _find_config_path(self) -> str:
        """æŸ¥æ‰¾é…ç½®æ–‡ä»¶è·¯å¾„"""
        # è·å– ai_service ç›®å½•
        ai_service_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        # é»˜è®¤é…ç½®æ–‡ä»¶è·¯å¾„ï¼šai_service/config/models.json
        default_config_path = os.path.join(ai_service_dir, 'config', 'models.json')
        
        # å¦‚æœé»˜è®¤è·¯å¾„å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if os.path.exists(default_config_path):
            return default_config_path
        
        # å°è¯•å…¶ä»–ä½ç½®
        possible_paths = [
            default_config_path,
            os.path.join(ai_service_dir, 'models.json'),
            os.path.join(os.path.dirname(ai_service_dir), 'ai_service', 'config', 'models.json'),
            'models.json',
        ]
        
        for path in possible_paths:
            abs_path = os.path.abspath(path)
            if os.path.exists(abs_path):
                return abs_path
        
        # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œè¿”å›é»˜è®¤è·¯å¾„ï¼ˆç”¨äºåˆ›å»ºé»˜è®¤é…ç½®ï¼‰
        return default_config_path
    
    def _load_models(self):
        """ä»é…ç½®æ–‡ä»¶åŠ è½½æ¨¡å‹"""
        if not os.path.exists(self.config_path):
            safe_print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}ï¼Œå°è¯•åˆ›å»ºé»˜è®¤é…ç½®")
            self._create_default_config()
            if not os.path.exists(self.config_path):
                safe_print(f"âŒ æ— æ³•åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶ï¼Œæ¨¡å‹åŠ è½½å¤±è´¥")
                return
        
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            models = config.get('models', [])
            
            for model_config in models:
                # è·³è¿‡æ³¨é‡Šå­—æ®µï¼ˆä»¥ _ å¼€å¤´çš„å­—æ®µï¼‰
                if isinstance(model_config, dict) and any(key.startswith('_') for key in model_config.keys()):
                    if not model_config.get('id'):
                        # è¿™æ˜¯ä¸€ä¸ªæ³¨é‡Šæ¡ç›®ï¼Œè·³è¿‡
                        continue
                
                # ç¡®ä¿æ˜¯å­—å…¸ç±»å‹
                if not isinstance(model_config, dict):
                    continue
                
                model_id = model_config.get('id')
                if not model_id:
                    # å¯èƒ½æ˜¯æ³¨é‡Šå­—æ®µï¼Œè·³è¿‡
                    continue
                
                adapter_type = model_config.get('adapter', 'openai_compat')
                enabled = model_config.get('enabled', True)
                
                if not enabled:
                    safe_print(f"â„¹ï¸ æ¨¡å‹ {model_id} å·²ç¦ç”¨ï¼Œè·³è¿‡")
                    continue
                
                try:
                    adapter = self._create_adapter(adapter_type, model_config)
                    if adapter and adapter.is_available():
                        self.adapters[model_id] = adapter
                        # è¾“å‡ºåˆ° stderr ä»¥ä¾¿è¢« Rust åç«¯æ•è·
                        print(f"âœ… æ¨¡å‹ {model_id} ({adapter_type}) å·²åŠ è½½", file=sys.stderr, flush=True)
                    else:
                        # è¾“å‡ºè¯¦ç»†ä¿¡æ¯å¸®åŠ©è°ƒè¯•
                        reason = []
                        if not adapter:
                            reason.append("é€‚é…å™¨åˆ›å»ºå¤±è´¥")
                        elif not adapter.is_available():
                            reason.append("é€‚é…å™¨ä¸å¯ç”¨")
                            # æ£€æŸ¥å…·ä½“åŸå› 
                            if hasattr(adapter, 'api_key') and not adapter.api_key:
                                reason.append("ç¼ºå°‘ API Key")
                            if hasattr(adapter, 'base_url') and not adapter.base_url:
                                reason.append("ç¼ºå°‘ Base URL")
                        # è¾“å‡ºåˆ° stderr ä»¥ä¾¿è¢« Rust åç«¯æ•è·
                        print(f"âš ï¸ æ¨¡å‹ {model_id} ({adapter_type}) ä¸å¯ç”¨ï¼Œè·³è¿‡ã€‚åŸå› : {', '.join(reason) if reason else 'æœªçŸ¥'}", file=sys.stderr, flush=True)
                except Exception as e:
                    print(f"âŒ åˆå§‹åŒ–æ¨¡å‹ {model_id} å¤±è´¥: {e}", file=sys.stderr, flush=True)
                    if config.get('debug', False):
                        traceback.print_exc(file=sys.stderr)
        
        except Exception as e:
            safe_print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)
            if config.get('debug', False):
                traceback.print_exc(file=sys.stderr)
    
    def _create_adapter(self, adapter_type: str, config: Dict) -> Optional[ChatAdapter]:
        """
        åˆ›å»ºé€‚é…å™¨å®ä¾‹
        å¯¹åº” One API çš„ relay.GetAdaptor
        """
        try:
            if adapter_type == 'openai_compat':
                return OpenAICompatAdapter(config)
            elif adapter_type == 'custom_http':
                return CustomHTTPAdapter(config)
            elif adapter_type == 'process':
                return ProcessAdapter(config)
            elif adapter_type == 'websocket' or adapter_type == 'websocket_xunfei':
                # æ ¹æ® request_format é€‰æ‹©å…·ä½“çš„ WebSocket é€‚é…å™¨
                request_format = config.get('request_format', '')
                if request_format == 'xunfei':
                    return XunfeiAdapter(config)
                else:
                    # é»˜è®¤ä½¿ç”¨è®¯é£é€‚é…å™¨ï¼ˆå‘åå…¼å®¹ï¼‰
                    return XunfeiAdapter(config)
            else:
                safe_print(f"âš ï¸ æœªçŸ¥çš„é€‚é…å™¨ç±»å‹: {adapter_type}")
                return None
        except Exception as e:
            safe_print(f"âŒ åˆ›å»ºé€‚é…å™¨å¤±è´¥: {e}", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
            return None
    
    def _create_default_config(self):
        """åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶"""
        default_config = {
            "models": [
                {
                    "id": "gpt-3.5-turbo",
                    "adapter": "openai_compat",
                    "base_url": "https://api.openai.com/v1",
                    "api_key": "ENV:OPENAI_API_KEY",
                    "enabled": True,
                    "model": "gpt-3.5-turbo",
                    "temperature": 0.7,
                    "max_tokens": 2000,
                    "timeout": 60
                },
                {
                    "id": "deepseek-chat",
                    "adapter": "openai_compat",
                    "base_url": "https://api.deepseek.com/v1",
                    "api_key": "ENV:DEEPSEEK_API_KEY",
                    "enabled": True,
                    "model": "deepseek-chat",
                    "temperature": 0.7,
                    "max_tokens": 2000,
                    "timeout": 60
                }
            ]
        }
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.config_path), exist_ok=True)
        
        try:
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            safe_print(f"âœ… å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {self.config_path}")
        except Exception as e:
            safe_print(f"âš ï¸ åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
    
    def get_adapter(self, model_id: str) -> Optional[ChatAdapter]:
        """
        è·å–æŒ‡å®šæ¨¡å‹çš„é€‚é…å™¨
        å¯¹åº” One API çš„ CacheGetRandomSatisfiedChannelï¼ˆç®€åŒ–ç‰ˆï¼Œæ— è´Ÿè½½å‡è¡¡ï¼‰
        """
        return self.adapters.get(model_id)
    
    def list_models(self) -> Dict[str, Any]:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼ˆOpenAI å…¼å®¹æ ¼å¼ï¼‰
        ç”¨äº /v1/models æ¥å£
        """
        models_info = [adapter.get_model_info() for adapter in self.adapters.values()]
        return {
            "object": "list",
            "data": models_info
        }
    
    def reload(self):
        """é‡æ–°åŠ è½½é…ç½®"""
        try:
            if sys.stdout and not sys.stdout.closed:
                print("ğŸ”„ é‡æ–°åŠ è½½æ¨¡å‹é…ç½®...", flush=True)
        except (ValueError, OSError, AttributeError):
            pass
        self.adapters.clear()
        self._load_models()

